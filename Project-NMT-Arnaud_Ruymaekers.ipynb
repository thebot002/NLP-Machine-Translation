{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98298dd0-7d68-462c-8a94-269465b27167",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "857b491c-2611-45d3-b64e-851d48338159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import spacy\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "\n",
    "nlp_models = {\n",
    "    'en': spacy.load('en_core_web_md'),\n",
    "    'it': spacy.load('it_core_news_md')\n",
    "}\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import Tree\n",
    "from nltk.translate import bleu_score as bleu\n",
    "from nltk.translate import IBMModel1, AlignedSent\n",
    "from spacy import displacy\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b9c50-fcf1-4dcd-8f37-edeb10a873ef",
   "metadata": {},
   "source": [
    "# NLP Final Project - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472aa4fd-9392-4dd9-b87c-ec2281950219",
   "metadata": {},
   "source": [
    "## Exploration of Machine Translation Techniques using Movie Subtitles dataset\n",
    "\n",
    "Arnaud Ruymaekers, S5298338"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9f262-9e1b-4360-81ab-7207aba54a90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Description: \n",
    "\n",
    "I would like to explore developping 3 different techniques to perform Machine Translation. \n",
    "I would like to implement and compare implementations of a Statistical, Rule-Based and Neural Machine Translation.\n",
    "I will attempt to implement these techniques from scratch (not using libraries to do the whole thing) to understand how they work on a deeper level.\n",
    "I plan to implement this in python and to use as dataset sentence correspondances from movies subtitles EN <-> IT coming from opensubtitles.org .\n",
    "\n",
    "Feedback:\n",
    "\n",
    "If you will develop 3 different techniques, the project will be for sure hard. As a B-plan, you might downgrade to developing 2 techniques only, \n",
    "to make sure to stay in about 7 to 10 days of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e868b-a7b5-4230-b320-f1875e837ef9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introduction (TODO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96440675-cc5d-4b5d-a2fa-922877f43ef1",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Datasets Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa13dbb4-4d76-4700-a511-00931c33e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count_total = 35_216_229\n",
    "file_name = 'OpenSubtitles.en-it.'\n",
    "languages = ['en', 'it']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70264a9-0048-4aa3-b062-56f4bd3c5658",
   "metadata": {},
   "source": [
    "### Text Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4097c139-a620-4701-8325-e204c67291df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(file_name, lang='en', line_count=None, from_line=0, tokenize=True) -> dict:\n",
    "    \n",
    "    if line_count is None:\n",
    "        line_count = line_count_total\n",
    "        \n",
    "    assert (from_line+line_count <= line_count_total), f'line_count + from_line should be under {line_count_total} (it is currently {line_count+from_line})'\n",
    "    \n",
    "    file_lines = []\n",
    "    \n",
    "    with ZipFile('en-it.txt.zip') as zf:\n",
    "        with zf.open(file_name + lang, 'r') as f:\n",
    "\n",
    "            for i, line in tqdm(enumerate(f), total=from_line+line_count, desc=f'Reading {lang.upper()} language file'):\n",
    "                if i < from_line:\n",
    "                    continue\n",
    "                elif i < from_line+line_count:\n",
    "                    decoded_line = line.decode(\"utf-8\").replace('\\n', '')\n",
    "                    file_lines.append(word_tokenize(decoded_line) if tokenize else decoded_line)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa778989-db05-4d4d-90e1-07e984e44516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89edeb42579d490b8d491747cbee4a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading EN language file:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71838e06ba143f58ba573f1cc690bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading IT language file:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting 100k sentences for now\n",
    "sentences = {}\n",
    "for lang in languages:\n",
    "    sentences[lang] = extract_file(file_name, lang, 100_000, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3af910-ce49-4029-baac-e858d63801fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\tPermaculture is a design science based on three simple ethics:\n",
      "\t\t=> La permacultura è un metodo di progettazione basato su tre semplici principi etici:\n",
      "Sample 1:\n",
      "\tcare for the earth\n",
      "\t\t=> cura della terra\n",
      "Sample 2:\n",
      "\tcare for people\n",
      "\t\t=> cura delle persone\n",
      "Sample 3:\n",
      "\tshare the surplus\n",
      "\t\t=> Condividi il superfluo\n",
      "Sample 4:\n",
      "\tPermaculture also has core principles They guide us in creating sustainable abundance\n",
      "\t\t=> La permacultura ha anche principi cardine le linee guida per la creazione di abbondanza sostenibile\n"
     ]
    }
   ],
   "source": [
    "# Printing some samples\n",
    "for i in range(5):\n",
    "    print(f'Sample {i}:')\n",
    "    print('\\t' + sentences['en'][i])\n",
    "    print('\\t\\t=> ' + sentences['it'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2289ead-094b-44cc-b0ce-d9cd553c5296",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611a95d2-25fb-47c9-abdc-f7ed96ac97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, lang='en'):\n",
    "    return [word_tokenize(sentence) for sentence in tqdm(sentences[lang], desc=f'Tokenizing {lang.upper()} doc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10db980f-b20d-4cd0-b56e-01ae6c5d3075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5250082068a84147a0f7a7a8508e2ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing EN doc:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d937378c72451fab9d5eacadb3b5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing IT doc:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_sentences = {}\n",
    "for lang in languages:\n",
    "    tok_sentences[lang] = tokenize_sentences(sentences, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d1d591-cb53-4c0f-8de0-c6393324be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "\t[Permaculture, is, a, design, science, based, on, three, simple, ethics, :]\n",
      "\t\t=> [La, permacultura, è, un, metodo, di, progettazione, basato, su, tre, semplici, principi, etici, :]\n",
      "Sample 1:\n",
      "\t[care, for, the, earth]\n",
      "\t\t=> [cura, della, terra]\n",
      "Sample 2:\n",
      "\t[care, for, people]\n",
      "\t\t=> [cura, delle, persone]\n",
      "Sample 3:\n",
      "\t[share, the, surplus]\n",
      "\t\t=> [Condividi, il, superfluo]\n",
      "Sample 4:\n",
      "\t[Permaculture, also, has, core, principles, They, guide, us, in, creating, sustainable, abundance]\n",
      "\t\t=> [La, permacultura, ha, anche, principi, cardine, le, linee, guida, per, la, creazione, di, abbondanza, sostenibile]\n"
     ]
    }
   ],
   "source": [
    "# Printing some samples\n",
    "for i in range(5):\n",
    "    print(f'Sample {i}:')\n",
    "    print('\\t[' + ', '.join(tok_sentences['en'][i]) + ']')\n",
    "    print('\\t\\t=> [' + ', '.join(tok_sentences['it'][i]) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae9f10-7310-4e52-8a50-e8ce0baab8fa",
   "metadata": {},
   "source": [
    "### Vocabulary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eaeee2b-7676-4a71-8f53-2d235d15de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(sentences, lang='en'):\n",
    "    vocab = set()\n",
    "    for sentence in tqdm(sentences[lang], desc=f'Vocab extraction for {lang.upper()}'):\n",
    "        vocab |= set(sentence)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292ec8ce-7054-465c-a08d-dfc527271b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dac07cbaac4a3bb76be2f4d9f28608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vocab extraction for EN:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32960\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db737645a9941d48ec72f8ec1a4e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vocab extraction for IT:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 45096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabs = {}\n",
    "for lang in languages:\n",
    "    vocabs[lang] = extract_vocab(tok_sentences, lang)\n",
    "    print(f'Vocab size: {len(vocabs[lang])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f1979-baec-4dc3-9d7f-ddf5fbfbf0e6",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0517f-8d20-4125-b76d-f675b286413c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f364f7-8b3b-48d6-9ffc-03d11bff2876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7f4455-f58f-468c-9cb4-b245ed2cd335",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5715220-42c0-4f39-b330-29c59813bb00",
   "metadata": {},
   "source": [
    "DS:\n",
    "- https://opus.nlpl.eu/OpenSubtitles.php\n",
    "- http://www.opensubtitles.org/\n",
    "\n",
    "General:\n",
    "- https://machinetranslate.org/\n",
    "- https://towardsdatascience.com/machine-translation-b0f0dbcef47c\n",
    "- https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a\n",
    "\n",
    "Evalutation:\n",
    "- https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1\n",
    "\n",
    "Neural model:\n",
    "- https://github.com/tensorflow/nmt#training--how-to-build-our-first-nmt-system\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
