{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98298dd0-7d68-462c-8a94-269465b27167",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857b491c-2611-45d3-b64e-851d48338159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import bleu_score as bleu\n",
    "from nltk.translate import IBMModel1, AlignedSent\n",
    "from tqdm.notebook import tqdm\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29952ab1-9750-4f6c-a5bc-e2ce4ef271a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberToBase(n, b, pos_count=1):\n",
    "    digits = []\n",
    "    while n or len(digits) < pos_count:\n",
    "        digits.append(int(n % b))\n",
    "        n //= b\n",
    "    return digits[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472aa4fd-9392-4dd9-b87c-ec2281950219",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "\n",
    "## Exploration of Machine Translation Techniques using Movie Subtitles dataset\n",
    "\n",
    "Arnaud Ruymaekers, S5298338"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9f262-9e1b-4360-81ab-7207aba54a90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Description: \n",
    "\n",
    "I would like to explore developping 3 different techniques to perform Machine Translation. \n",
    "I would like to implement and compare implementations of a Statistical, Rule-Based and Neural Machine Translation.\n",
    "I will attempt to implement these techniques from scratch (not using libraries to do the whole thing) to understand how they work on a deeper level.\n",
    "I plan to implement this in python and to use as dataset sentence correspondances from movies subtitles EN <-> IT coming from opensubtitles.org .\n",
    "\n",
    "Feedback:\n",
    "\n",
    "If you will develop 3 different techniques, the project will be for sure hard. As a B-plan, you might downgrade to developing 2 techniques only, \n",
    "to make sure to stay in about 7 to 10 days of work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e868b-a7b5-4230-b320-f1875e837ef9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introduction (TODO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96440675-cc5d-4b5d-a2fa-922877f43ef1",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Datasets Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa13dbb4-4d76-4700-a511-00931c33e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count_total = 35_216_229\n",
    "file_name = 'OpenSubtitles.en-it.'\n",
    "languages = ['en', 'it']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70264a9-0048-4aa3-b062-56f4bd3c5658",
   "metadata": {},
   "source": [
    "### Text Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4097c139-a620-4701-8325-e204c67291df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(languages=['en', 'it'], line_count=None, from_line=0, tokenize=True) -> dict:\n",
    "        \n",
    "    if line_count is None:\n",
    "        line_count = line_count_total\n",
    "        \n",
    "    assert (from_line+line_count <= line_count_total), f'line_count + from_line should be under {line_count_total} (it is currently {line_count+from_line})'\n",
    "    \n",
    "    file_lines = {}\n",
    "    \n",
    "    with ZipFile('en-it.txt.zip') as zf:\n",
    "        \n",
    "        for lang in languages:\n",
    "            lines = []\n",
    "            with zf.open('OpenSubtitles.en-it.' + lang, 'r') as f:\n",
    "\n",
    "                for i, line in tqdm(enumerate(f), total=from_line+line_count, desc=f'Reading {lang.upper()} language file'):\n",
    "                    if i < from_line:\n",
    "                        continue\n",
    "                    elif i < from_line+line_count:\n",
    "                        decoded_line = line.decode(\"utf-8\").replace('\\n', '')\n",
    "                        lines.append(word_tokenize(decoded_line) if tokenize else decoded_line)\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "            file_lines[lang] = lines\n",
    "\n",
    "    return file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa778989-db05-4d4d-90e1-07e984e44516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c328e43308347cbb826c7c114cf785a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading EN language file:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e0308cf0544765a820e91e3ec30802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading IT language file:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting 100k sentences for now\n",
    "sentences = extract_file(languages, 100_000, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3af910-ce49-4029-baac-e858d63801fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "\tPermaculture is a design science based on three simple ethics:\n",
      "\t\t=> La permacultura è un metodo di progettazione basato su tre semplici principi etici:\n",
      "Sample 2:\n",
      "\tcare for the earth\n",
      "\t\t=> cura della terra\n",
      "Sample 3:\n",
      "\tcare for people\n",
      "\t\t=> cura delle persone\n",
      "Sample 4:\n",
      "\tshare the surplus\n",
      "\t\t=> Condividi il superfluo\n",
      "Sample 5:\n",
      "\tPermaculture also has core principles They guide us in creating sustainable abundance\n",
      "\t\t=> La permacultura ha anche principi cardine le linee guida per la creazione di abbondanza sostenibile\n"
     ]
    }
   ],
   "source": [
    "# Printing some samples\n",
    "for i in range(5):\n",
    "    print(f'Sample {i+1}:')\n",
    "    print('\\t' + sentences['en'][i])\n",
    "    print('\\t\\t=> ' + sentences['it'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab12a95",
   "metadata": {},
   "source": [
    "Sample 1:\n",
    "\tPermaculture is a design science based on three simple ethics:\n",
    "\t\t=> La permacultura è un metodo di progettazione basato su tre semplici principi etici:\n",
    "Sample 2:\n",
    "\tcare for the earth\n",
    "\t\t=> cura della terra\n",
    "Sample 3:\n",
    "\tcare for people\n",
    "\t\t=> cura delle persone\n",
    "Sample 4:\n",
    "\tshare the surplus\n",
    "\t\t=> Condividi il superfluo\n",
    "Sample 5:\n",
    "\tPermaculture also has core principles They guide us in creating sustainable abundance\n",
    "\t\t=> La permacultura ha anche principi cardine le linee guida per la creazione di abbondanza sostenibile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2289ead-094b-44cc-b0ce-d9cd553c5296",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611a95d2-25fb-47c9-abdc-f7ed96ac97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tok_sentences = {}\n",
    "    for lang, lang_sentences in sentences.items():\n",
    "        tok_sentences[lang] = [word_tokenize(sentence) for sentence in tqdm(lang_sentences, desc=f'Tokenizing {lang.upper()} doc')]\n",
    "    return tok_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10db980f-b20d-4cd0-b56e-01ae6c5d3075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a0fc4a7ce4fb49bd0d94c03154b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing EN doc:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024d822b5b8e4e49bab315d6acdeed70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing IT doc:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_tok_sentences = tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d1d591-cb53-4c0f-8de0-c6393324be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "\t[Permaculture, is, a, design, science, based, on, three, simple, ethics, :]\n",
      "\t\t=> [La, permacultura, è, un, metodo, di, progettazione, basato, su, tre, semplici, principi, etici, :]\n",
      "Sample 2:\n",
      "\t[care, for, the, earth]\n",
      "\t\t=> [cura, della, terra]\n",
      "Sample 3:\n",
      "\t[care, for, people]\n",
      "\t\t=> [cura, delle, persone]\n",
      "Sample 4:\n",
      "\t[share, the, surplus]\n",
      "\t\t=> [Condividi, il, superfluo]\n",
      "Sample 5:\n",
      "\t[Permaculture, also, has, core, principles, They, guide, us, in, creating, sustainable, abundance]\n",
      "\t\t=> [La, permacultura, ha, anche, principi, cardine, le, linee, guida, per, la, creazione, di, abbondanza, sostenibile]\n"
     ]
    }
   ],
   "source": [
    "# Printing some samples\n",
    "for i in range(5):\n",
    "    print(f'Sample {i+1}:')\n",
    "    print('\\t[' + ', '.join(raw_tok_sentences['en'][i]) + ']')\n",
    "    print('\\t\\t=> [' + ', '.join(raw_tok_sentences['it'][i]) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68701a0c-4e89-4210-a6d7-c3dd738223d4",
   "metadata": {},
   "source": [
    "### Filtering tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61aeefc-c4be-441e-ac36-27c57133edba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Permaculture', 'is', 'a', 'design', 'science', 'based', 'on', 'three', 'simple', 'ethics', ':']\n",
      "['La', 'permacultura', 'è', 'un', 'metodo', 'di', 'progettazione', 'basato', 'su', 'tre', 'semplici', 'principi', 'etici', ':']\n"
     ]
    }
   ],
   "source": [
    "for en_sent, it_sent in zip(*[raw_tok_sentences[lang] for lang in languages]):\n",
    "    print(en_sent)\n",
    "    print(it_sent)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71ad32c-5b07-49a8-915d-b7ddb316a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):    \n",
    "    cleaned_sentence = []\n",
    "    for word in sentence:\n",
    "        lower_word = word.lower()\n",
    "        punc_less_word = re.sub(r'[^\\w\\s]', '', lower_word)\n",
    "        \n",
    "        if len(punc_less_word) > 0:\n",
    "            cleaned_sentence.append(punc_less_word)\n",
    "            \n",
    "    return cleaned_sentence\n",
    "\n",
    "\n",
    "def clean_all_sentences(sentences):\n",
    "    tok_sentences = {'en': [], 'it':[]}\n",
    "    for en_sent, it_sent in tqdm(zip(*[raw_tok_sentences[lang] for lang in languages]), total=len(raw_tok_sentences['en']), desc='Filtering sentences'):\n",
    "        filt_en_sent = clean_sentence(en_sent)\n",
    "        filt_it_sent = clean_sentence(it_sent)\n",
    "        \n",
    "        if len(filt_en_sent) > 0 and len(filt_it_sent) > 0:\n",
    "            tok_sentences['en'].append(filt_en_sent)\n",
    "            tok_sentences['it'].append(filt_it_sent)\n",
    "            \n",
    "    return tok_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c39d78-1470-44c5-874f-0b6ca74d4b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867bda95c7784c118139259c458576e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering sentences:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_sentences = clean_all_sentences(raw_tok_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae9f10-7310-4e52-8a50-e8ce0baab8fa",
   "metadata": {},
   "source": [
    "### Vocabulary Extraction (UNUSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eaeee2b-7676-4a71-8f53-2d235d15de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(sentences):\n",
    "    vocabs = {'en':set(), 'it':set()}\n",
    "    \n",
    "    for lang, lang_sentences in sentences.items():\n",
    "        for sentence in tqdm(lang_sentences, desc=f'Vocab extraction for {lang.upper()}'):\n",
    "            vocabs[lang] |= set(sentence)\n",
    "        print(f'Vocab size: {len(vocabs[lang])}\\n')\n",
    "        \n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292ec8ce-7054-465c-a08d-dfc527271b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7e9d44778d4f7088a50d089b57ff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vocab extraction for EN:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25652\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787ca0bad3504725beeae310856c6f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vocab extraction for IT:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 37470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocabs = extract_vocab(tok_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c311f4-e5ba-45c9-8cca-da6afb800fe7",
   "metadata": {},
   "source": [
    "### Filtering to short sentences (UNUSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5005042-bdfd-4916-9c16-b0a38a0837ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sent_to_len(sentences, l=15):\n",
    "    short_sentences = {'en': [], 'it': []}\n",
    "\n",
    "    n = len(sentences['en'])\n",
    "    new_n = 0\n",
    "    \n",
    "    for i in tqdm(range(n), desc='Filtering to short sentences'):\n",
    "        en_sent = sentences['en'][i]\n",
    "        it_sent = sentences['it'][i]\n",
    "\n",
    "        if len(en_sent) <= l and len(it_sent) <= l:\n",
    "            short_sentences['en'].append(en_sent)\n",
    "            short_sentences['it'].append(it_sent)\n",
    "            new_n += 1\n",
    "            \n",
    "    print(f'There are {new_n} sentences left with length {l} or less')\n",
    "    \n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce18c0b7-473d-49e2-82b4-c3b6e3507667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2226602949334657986750a5cfdef22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering to short sentences:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78513 sentences left with length 10 or less\n"
     ]
    }
   ],
   "source": [
    "short_tok_sentences = filter_sent_to_len(tok_sentences, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8900b-d1da-4703-a56f-e38fb3a7bec2",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f72ee7-d8c2-4ef3-a844-ec66ba3198cc",
   "metadata": {},
   "source": [
    "### Retrieving N-gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9414c67-c679-4d09-8560-4d4c4bbf8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_counts(sentences, n_s=[1,2,3,4]):\n",
    "    ngram_counts = {}\n",
    "    \n",
    "    for lang, lang_sentences in sentences.items():\n",
    "        lang_ngram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for sentence in tqdm(lang_sentences, desc=f'Retrieving n-gram counts for {lang.upper()} doc'):\n",
    "            sentence = (['<start>'] * (max(n_s) - 1)) + sentence + ['<end>']\n",
    "            for n in n_s:\n",
    "                for i in range(len(sentence)-n+1):\n",
    "                    ngram = tuple(sentence[i:i+n])\n",
    "                    lang_ngram_counts[n][ngram] += 1\n",
    "                    \n",
    "        ngram_counts[lang] = lang_ngram_counts\n",
    "        \n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9440f1ea-40cc-485d-9c49-b714f38962d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cc7b60ecf34b4b93bf9cdd9b066ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving n-gram counts for EN doc:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b25c8a6c5844e58158c8be30708be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving n-gram counts for IT doc:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_counts =  build_ngram_counts(tok_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa315e-7dfd-4575-b24e-8a796fe97563",
   "metadata": {},
   "source": [
    "### Computing N-gram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39746d7c-d1f5-474b-bfc1-a0058b6a0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_probs(ngram_counts, lang='en'):\n",
    "    probs = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "    for n, counts in tqdm(ngram_counts.items(), desc=f'Building n-gram probabilities for {lang.upper()}'):\n",
    "        \n",
    "        if n > 1:\n",
    "            for ngram, count in tqdm(counts.items(), leave=False):\n",
    "                if ngram_counts[n-1][tuple(ngram[:-1])] == 0.0:\n",
    "                    print(ngram[:-1])\n",
    "                probs[tuple(ngram[:-1])][ngram[-1]] = count / ngram_counts[n-1][tuple(ngram[:-1])]\n",
    "                \n",
    "        else:\n",
    "            total = sum(list(counts.values()))\n",
    "            for ngram, count in tqdm(counts.items(), leave=False):\n",
    "                probs[tuple()][ngram[0]] = count / total\n",
    "            \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49b51fb0-b576-4b3b-9c30-276b461b5be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc24dd119ab445749a2b0c41c8dd74ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building n-gram probabilities for EN:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/425218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/532251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36a189ed98e448ebd54205aebcd8dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building n-gram probabilities for IT:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/436752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_probs = {}\n",
    "for lang in languages:\n",
    "    ngram_probs[lang] = build_ngram_probs(ngram_counts[lang], lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27d818-fede-471d-8f40-019d9a4c3c0d",
   "metadata": {},
   "source": [
    "### Language Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d655129e-4f95-4181-91f0-6df146c0fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def sequence_probability(self, sequence):\n",
    "        pass\n",
    "    \n",
    "    def next_word_from_sequence(self, sequence):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01c55374-35c5-44c6-a76f-342f22998fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7328f24560c64843940454101c754020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unigrams probabilities:   0%|          | 0/25654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beffb7453d6c40bbabfa6b01f98268a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unigrams probabilities:   0%|          | 0/37472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class UnigramModel(LanguageModel):\n",
    "    def __init__(self, counts):\n",
    "        self.probabilities = defaultdict(lambda: 0.0)\n",
    "        \n",
    "        total = sum(list(counts[1].values()))\n",
    "        for ngram, count in tqdm(counts[1].items(), desc='Generating unigrams probabilities'):\n",
    "            self.probabilities[ngram[0]] = count / total\n",
    "            \n",
    "    def sequence_probability(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            sequence = sequence[-1]\n",
    "        return self.probabilities[sequence]\n",
    "    \n",
    "    def next_word_from_sequence(self, sequence):\n",
    "        return self.probabilities\n",
    "    \n",
    "unigram_models = {\n",
    "    'en': UnigramModel(ngram_counts['en']),\n",
    "    'it': UnigramModel(ngram_counts['it'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a88448f-1bf2-4038-92cb-17472b520ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06114274342e45c0949ede418f5c37d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 2-gram probabilities:   0%|          | 0/206287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5705397f0af44abbf6cfbe464319d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 2-gram probabilities:   0%|          | 0/242613 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777956e03be148d184d84776f28f3f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 3-gram probabilities:   0%|          | 0/425218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9a3b758b3847b3940a476def72d443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 3-gram probabilities:   0%|          | 0/436752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54164f6b01fe4677914f299c31c07156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 4-gram probabilities:   0%|          | 0/532251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c47551e0fe4e08a8eed2a2df30012b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 4-gram probabilities:   0%|          | 0/506352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NgramModel(LanguageModel):\n",
    "    def __init__(self, counts, n):\n",
    "        self.probabilities = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        self.n = n\n",
    "\n",
    "        for ngram, count in tqdm(counts[n].items(), desc=f'Generating {n}-gram probabilities'):\n",
    "            self.probabilities[tuple(ngram[:-1])][ngram[-1]] = count / counts[n-1][tuple(ngram[:-1])]\n",
    "            \n",
    "    def sequence_probability(self, sequence:list):\n",
    "        return self.probabilities[tuple(sequence[-n:-1])][-1]\n",
    "    \n",
    "    def next_word_from_sequence(self, sequence):\n",
    "        return self.probabilities[-(n-1):]\n",
    "    \n",
    "# 2-gram\n",
    "bigram_models = {\n",
    "    'en': NgramModel(ngram_counts['en'], 2),\n",
    "    'it': NgramModel(ngram_counts['it'], 2)\n",
    "}\n",
    "\n",
    "# 3-gram\n",
    "trigram_models = {\n",
    "    'en': NgramModel(ngram_counts['en'], 3),\n",
    "    'it': NgramModel(ngram_counts['it'], 3)\n",
    "}\n",
    "\n",
    "# 4-gram\n",
    "quadrigram_models = {\n",
    "    'en': NgramModel(ngram_counts['en'], 4),\n",
    "    'it': NgramModel(ngram_counts['it'], 4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519c7a4a-a095-4928-aa54-cdeb627f052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackoffModel(LanguageModel):\n",
    "    def __init__(self, counts, n, ngram_models, discount=0.6):\n",
    "        self.counts = counts\n",
    "        self.n = n\n",
    "        self.ngram_models = ngram_models\n",
    "    \n",
    "    def sequence_probability(self, sequence:list, investigating_n=None):\n",
    "        if not investigating_n:\n",
    "            investigating_n = self.n\n",
    "        \n",
    "        if self.counts[investigating_n][sequence[-n:]] > 0 or len(sequence) < 2:\n",
    "            return ngram_models[investigating_n-1].sequence_probability(sequence)\n",
    "        else:\n",
    "            return self.sequence_probability(sequence[-(investigating_n-1):], investigating_n=(investigating_n-1))\n",
    "    \n",
    "    def next_word_from_sequence(self, sequence):\n",
    "        return self.ngram_models[n-1](sequence)\n",
    "\n",
    "backoff_models = {}\n",
    "\n",
    "for lang in languages:\n",
    "    ngram_models = [unigram_models[lang], bigram_models[lang], trigram_models[lang]]\n",
    "    backoff_models[lang] = BackoffModel(ngram_counts, 3, ngram_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8245f05-8290-48b8-8ec1-dce3cb2fa0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_lambdas(sentences, ngrams, max_ngram, lang='en', full_print=False):\n",
    "#     assert max_ngram > 1, 'max_ngram should be at least 2'\n",
    "#     assert max_ngram <= 10, 'max_gram cant be above 10' \n",
    "    \n",
    "#     l_options = [(i+1)/10 for i in range(10-(max_ngram-1))]\n",
    "#     combinations = len(l_options)**(max_ngram-1)\n",
    "    \n",
    "#     best_lambdas = None\n",
    "#     best_prob = float('-inf')\n",
    "    \n",
    "#     for comb in tqdm(range(combinations), desc=f'Checking lambda sets for {lang.upper()}'):\n",
    "#         # Compute lambda option\n",
    "#         indices = numberToBase(comb, len(l_options), (max_ngram-1))\n",
    "        \n",
    "#         lambdas = [l_options[ind] for ind in indices]\n",
    "#         last_l = round((1.0 - sum(lambdas)), 1)\n",
    "        \n",
    "#         if last_l < 0.1:\n",
    "#             continue\n",
    "        \n",
    "#         lambdas += [last_l]\n",
    "        \n",
    "#         # print(f'{lambdas} = {round(sum(lambdas), 1)}')\n",
    "        \n",
    "#         # Computing sentence set probabilities\n",
    "#         total_prob = 0.0\n",
    "#         prob_count = 0\n",
    "    \n",
    "#         for sent in tqdm(sentences, desc=f'Lamdba set {lambdas}', leave=full_print):\n",
    "#             extended_sent = (['<start>'] * (max_ngram-1)) + sent + ['<end>']\n",
    "            \n",
    "#             sent_prob = 1\n",
    "#             for i in range((max_ngram-1), len(sent) + (max_ngram-1) + 1):\n",
    "#                 sent_prob *= sum([l*ngrams[tuple(extended_sent[i-j:i])][extended_sent[i]] for j, l in enumerate(lambdas)])\n",
    "                \n",
    "#             total_prob += sent_prob\n",
    "#             prob_count += 1\n",
    "        \n",
    "#         # Deciding whether to lambda set is better than previous best\n",
    "#         average_prob = total_prob / prob_count\n",
    "        \n",
    "#         if average_prob > best_prob:\n",
    "#             best_prob = average_prob\n",
    "#             best_lambdas = lambdas\n",
    "                \n",
    "#         if full_print:\n",
    "#             print(average_prob)\n",
    "            \n",
    "#     return best_lambdas\n",
    "\n",
    "# ngram_lambdas = {}\n",
    "\n",
    "# for lang in languages:\n",
    "#     lambdas = find_lambdas(tok_sentences['en'], ngram_probs['en'], 3)\n",
    "#     print(f'Best lambdas found: {lambdas}')\n",
    "#     ngram_lambdas[lang] = lambdas\n",
    "    \n",
    "# ngram_lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02c79a-ec21-42e9-ba5b-9572906754ad",
   "metadata": {},
   "source": [
    "## Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bd1da-b944-461e-958a-4876d3b46432",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### IBM Model 1\n",
    "\n",
    "- estimating t-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cbafa5f-e4a8-4de2-92e4-384e22f258a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model1(tok_sentences, source_lang='en', target_lang='it', max_epochs=20, t=None, show_sub_progress=True):\n",
    "    source_sentences = tok_sentences[source_lang]\n",
    "    target_sentences = tok_sentences[target_lang]\n",
    "\n",
    "    n = len(source_sentences)\n",
    "    \n",
    "    # Translation table\n",
    "    if not t:\n",
    "        t = defaultdict(lambda: defaultdict(lambda: random.random()))\n",
    "    else:\n",
    "        t = t.copy()\n",
    "\n",
    "    for epoch in tqdm(range(max_epochs), desc='Model training'):\n",
    "\n",
    "        counts_target_source = defaultdict(lambda: 0)\n",
    "        counts_target = defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "        for k in tqdm(range(n), desc=f'Epoch {epoch+1}', leave=show_sub_progress):\n",
    "            source_sent = source_sentences[k]\n",
    "            target_sent = [None] + target_sentences[k]\n",
    "\n",
    "            # Loop through source sentence words\n",
    "            for source_word in source_sent:\n",
    "\n",
    "                target_t_sum = sum([(t[source_word][target_word]) for target_word in target_sent])\n",
    "\n",
    "                # Loop through target sentence words\n",
    "                for target_word in target_sent:\n",
    "\n",
    "                    delta = t[source_word][target_word] / target_t_sum\n",
    "\n",
    "                    # Counts updates\n",
    "                    counts_target_source[(target_word, source_word)] += delta\n",
    "                    counts_target[target_word] += delta\n",
    "\n",
    "        # Update of ts\n",
    "        diff_sum = 0\n",
    "        diff_count = 0\n",
    "        for (target_word, source_word) in counts_target_source:\n",
    "            new_t = counts_target_source[(target_word, source_word)] / counts_target[target_word]\n",
    "\n",
    "            diff_sum += abs(new_t - t[source_word][target_word])\n",
    "            diff_count += 1\n",
    "\n",
    "            t[source_word][target_word] = new_t\n",
    "\n",
    "        # Computing avg_diff of the t values and decision of convergence\n",
    "        avg_diff = diff_sum/diff_count\n",
    "        \n",
    "        if show_sub_progress:\n",
    "            print(f'\\t - Average difference in t-values: {avg_diff}\\n')\n",
    "\n",
    "        if avg_diff < 0.0001:\n",
    "            print('Early exit because change between average t-table value changes is lower than 10-4')\n",
    "            break\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73f3184e-334b-4980-af4a-ed16d69fe171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cab7c37229a4a3f9d1cf171a3a9a16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab059d66e2034507bd9c1a52671e047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.48281345017381705\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9dc3f6df83477ca35eabe1d80b45f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.007517830520437741\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16daf4311b64afa8a410a2886c95f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.005363587539161923\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed374de182134dd08dd70ab529c8e3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0031112834255788853\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ea461b40974344b8eaedcc69ecd219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0018187764202964743\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fef4292091d4fe5bd9d9f22ba48c45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.001143428512975153\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a941a31f784957a233134519b05cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0007715040933502066\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bebd5b5346421e8b5860318a121daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0005512419148789678\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ef7048554846bb95c58c6625e1dc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0004113286976775883\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e3538968974d08b547a44805ecfe36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in t-values: 0.0003171680192572844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = ibm_model1(tok_sentences, 'en', 'it', 10, show_sub_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e60b1dbb-3cd2-4175-b0cb-9a4ea90f8a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permaculture > None - 5.069516994067443e-38\n",
      "permaculture > la - 2.127513906465088e-32\n",
      "permaculture > permacultura - 0.8721630570216184\n",
      "permaculture > è - 2.466527574822481e-31\n",
      "permaculture > un - 1.3480397932470805e-33\n",
      "permaculture > metodo - 4.619957815484537e-11\n",
      "permaculture > di - 3.392237325314037e-32\n",
      "permaculture > progettazione - 4.834074748069685e-05\n",
      "permaculture > basato - 2.5293415424010165e-13\n",
      "permaculture > su - 5.6962096646072605e-27\n",
      "permaculture > tre - 2.4920819342961294e-26\n"
     ]
    }
   ],
   "source": [
    "# Printing sample values\n",
    "source_word = list(t.keys())[0]\n",
    "for i, target_word in enumerate(t[source_word]):\n",
    "    print(f'{source_word} > {target_word} - {t[source_word][target_word]}')\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29142ca7-2af7-4967-8f23-a9b9372c1973",
   "metadata": {},
   "source": [
    "#### NLTK implementation comparisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "492a1e80-e57b-4af7-b6be-f31c8ee94900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitext = []\n",
    "# for i, source_sent in enumerate(tok_sentences['en']):\n",
    "#     bitext.append(AlignedSent(source_sent, tok_sentences['it'][i]))\n",
    "\n",
    "# nltk_ibm1 = IBMModel1(bitext, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cbbe283-5398-4eae-8d78-1d22e7b7bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing sample values\n",
    "# source_word = 'Permaculture'\n",
    "# for i, target_word in enumerate(nltk_ibm1.translation_table[source_word]):\n",
    "#     print(f'{source_word} > {target_word} - {nltk_ibm1.translation_table[source_word][target_word]}')\n",
    "#     if i >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63433047-4374-4c0a-8b8c-f476c3ddb017",
   "metadata": {},
   "source": [
    "### IBM Model 2\n",
    "\n",
    "- Estimating q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29b94544-e5a8-4b6e-b863-212953565f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model2(tok_sentences, source_lang='en', target_lang='it', max_epochs=20, t=None, q=None):\n",
    "    \n",
    "    # Source // corpus\n",
    "    source_sentences = tok_sentences[source_lang]\n",
    "    target_sentences = tok_sentences[target_lang]\n",
    "    \n",
    "    n = len(source_sentences)\n",
    "    \n",
    "    # Translation table\n",
    "    if not t:\n",
    "        t = defaultdict(lambda: defaultdict(lambda: random.random()))\n",
    "    else:\n",
    "        t = t.copy()\n",
    "    \n",
    "    # Alignment table\n",
    "    if not q:\n",
    "        q = defaultdict(lambda: defaultdict(lambda: random.random()))\n",
    "    else:\n",
    "        q = q.copy()\n",
    "    \n",
    "    # Do the iterations\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        counts_target_source = defaultdict(lambda: 0)\n",
    "        counts_target = defaultdict(lambda: 0)\n",
    "        counts_j_i_l_m = defaultdict(lambda: 0)\n",
    "        counts_i_l_m = defaultdict(lambda: 0)\n",
    "\n",
    "        # Iterate the corpus\n",
    "        for k in tqdm(range(n), desc=f'Epoch {epoch+1}'):\n",
    "            source_sent = [None] + source_sentences[k]\n",
    "            target_sent = ['UNUSED'] + target_sentences[k]\n",
    "\n",
    "            m = len(source_sent) - 1\n",
    "            l = len(target_sent) - 1\n",
    "\n",
    "            # Loop through source sentence words\n",
    "            for i in range(1, m+1):\n",
    "                source_word = source_sent[i]\n",
    "\n",
    "                target_t_q_sum = sum([(q[(i,l,m)][j] * t[source_word][target_sent[j]]) for j in range(0, l+1)])\n",
    "\n",
    "                # Loop through target sentence words\n",
    "                for j in range(0, l+1):\n",
    "                    target_word = target_sent[j]\n",
    "\n",
    "                    delta = (q[(i,l,m)][j] * t[source_word][target_word]) / target_t_q_sum\n",
    "\n",
    "                    # Counts updates\n",
    "                    counts_target_source[(target_word, source_word)] += delta\n",
    "                    counts_target[target_word] += delta\n",
    "                    counts_j_i_l_m[(j,i,l,m)] += delta\n",
    "                    counts_i_l_m[(i,l,m)] += delta\n",
    "\n",
    "        # Update of ts and qs\n",
    "        for (target_word, source_word) in counts_target_source:\n",
    "            t[source_word][target_word] = counts_target_source[(target_word, source_word)] / counts_target[target_word]\n",
    "\n",
    "        diff_sum = 0\n",
    "        diff_count = 0\n",
    "        for (j,i,l,m) in counts_j_i_l_m:\n",
    "            new_q = counts_j_i_l_m[(j,i,l,m)] / counts_i_l_m[(i,l,m)]\n",
    "\n",
    "            diff_sum += abs(new_q - q[(i,l,m)][j])\n",
    "            diff_count += 1\n",
    "\n",
    "            q[(i,l,m)][j] = new_q\n",
    "\n",
    "        # Computing avg_diff of the t values and decision of convergence\n",
    "        avg_diff = diff_sum/diff_count\n",
    "        print(f'\\t - Average difference in q-values: {avg_diff}\\n')\n",
    "\n",
    "        if avg_diff < 0.0001:\n",
    "            break\n",
    "            \n",
    "    return (t,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4594be98-ecce-4239-b7f7-3bfd27544639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b89e88efe44680a7b21a76b3436726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.4690395302357117\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c37abe80f9043ccaa2ce3f1720a0be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.03201975331755284\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcce3688852e42d68bb912bf251207ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.009113779279865033\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4a72282ac04a1a8ee22148b754b107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.004385103305592243\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8047f7cce24434bf8d5201df84c7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.0026033528885994876\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87d0d5b315042088d109c2e9649d200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.0017267162205747183\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47a9f2f25794f3c8efde5fb3d2fd9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.0012231865199606863\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d11b6fc5a94bd68c88b58e98f13173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.0009065247439692799\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ceb248a364675ad36f552af79a58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.0006982660139370228\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1a90e0e32e439bb597b5c627bab3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/99866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - Average difference in q-values: 0.000562269209468171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(_,q) = ibm_model2(tok_sentences, 'en', 'it', 10, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aec7af98-009e-43ed-b2a7-70ef895a87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 10) > 0 - 0.0002442271908711633\n",
      "(1, 13, 10) > 1 - 0.6384164863664822\n",
      "(1, 13, 10) > 2 - 0.12619653962393293\n",
      "(1, 13, 10) > 3 - 0.1054804613552266\n",
      "(1, 13, 10) > 4 - 0.0010699306558235504\n",
      "(1, 13, 10) > 5 - 0.05090313257603431\n",
      "(1, 13, 10) > 6 - 0.010280483800826018\n",
      "(1, 13, 10) > 7 - 0.0001481743321103478\n",
      "(1, 13, 10) > 8 - 4.3147107098094865e-07\n",
      "(1, 13, 10) > 9 - 0.004104445922785928\n",
      "(1, 13, 10) > 10 - 0.04773139541152379\n"
     ]
    }
   ],
   "source": [
    "# Printing sample values\n",
    "key_0 = list(q.keys())[0]\n",
    "for i, j in enumerate(q[key_0]):\n",
    "    print(f'{key_0} > {j} - {q[key_0][j]}')\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579ac2c-f142-437b-bca2-5cd3e1dadfd3",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53f23093-b6c0-4e92-b4af-242ebb035372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    def __init__(self,\n",
    "                 translated_sequence=['<start>','<start>'],\n",
    "                 probability=1.0,\n",
    "                 already_translated=[]\n",
    "                ):\n",
    "        self.translated_sequence = translated_sequence\n",
    "        self.probability = probability\n",
    "        self.already_translated = already_translated\n",
    "    \n",
    "    def add_word(self, word, word_prob, from_index):\n",
    "        return Hypothesis(\n",
    "                 translated_sequence = self.translated_sequence + [word],\n",
    "                 probability = self.probability * word_prob,\n",
    "                 already_translated = self.already_translated + [from_index]\n",
    "        )\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.probability}:' + ' '.join(self.translated_sequence[2:])\n",
    "\n",
    "    \n",
    "def get_ngram_prob(ngram_probs, sequence, discount=0.6, min_prob=0.0):\n",
    "    prob = ngram_probs[tuple(sequence[:-1])][sequence[-1]]\n",
    "    if prob > min_prob or len(sequence) < 2:\n",
    "        return prob\n",
    "    else:\n",
    "        return discount * get_ngram_prob(ngram_probs, sequence[1:], discount, min_prob)\n",
    "\n",
    "\n",
    "def beam_translate(sent, t, q, ngram_probs, beta, min_prob=0.0, max_options=None):\n",
    "    \n",
    "    source_sent_length = len(sent)\n",
    "    \n",
    "    # Retrieving all translation probabilities for the words in the source sequence\n",
    "    translation_probs = [t[word] for word in sent]\n",
    "    translation_probs = [dict(filter(lambda x: x[1]>min_prob, translation.items())) for translation in translation_probs]\n",
    "    if max_options:\n",
    "        translation_probs = [dict(sorted(translation.items(), key = lambda item: item[1], reverse = True)[:max_options]) for translation in translation_probs]\n",
    "    \n",
    "    # Retrieving distorition probs\n",
    "    distorted_translation_prob = [[{ word: (word_prob * q[(i+1, source_sent_length, source_sent_length)][j+1])\n",
    "                                    for word, word_prob in translation_dict.items()} \n",
    "                                   for i, translation_dict in enumerate(translation_probs)] \n",
    "                                  for j in range(source_sent_length)]\n",
    "    \n",
    "    # Defining the hypotheses list with an initial empty hypothesis\n",
    "    current_hypotheses = [Hypothesis()]\n",
    "    \n",
    "    for j in range(source_sent_length):\n",
    "        \n",
    "        # translation_probs = distorted_translation_prob[j]\n",
    "        new_hypotheses = []\n",
    "        \n",
    "        for hypothesis in current_hypotheses:\n",
    "            for i, translation_dict in enumerate(distorted_translation_prob[j]):\n",
    "                if i in hypothesis.already_translated:\n",
    "                    continue\n",
    "                \n",
    "                for word, translation_prob in translation_dict.items():\n",
    "                    \n",
    "                    # language model probabilities (tri-gram) \n",
    "                    lang_prob = get_ngram_prob(ngram_probs, hypothesis.translated_sequence[-2:] + [word], discount=0.6, min_prob=min_prob)\n",
    "\n",
    "                    # Combine probabilities and generate new hypothesis\n",
    "                    new_hypothesis = hypothesis.add_word(\n",
    "                        word=word,\n",
    "                        word_prob=(translation_prob * lang_prob),\n",
    "                        from_index=i\n",
    "                    )\n",
    "                    new_hypotheses.append(new_hypothesis)\n",
    "        \n",
    "        # Sorting options by probability\n",
    "        new_hypotheses = sorted(new_hypotheses, key=lambda hyp: hyp.probability, reverse=True)\n",
    "\n",
    "        # Taking \"beta\" best translations\n",
    "        current_hypotheses = new_hypotheses[:beta]\n",
    "                \n",
    "    return current_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d803e5fa-58e2-48a9-a38a-07fd2fd3ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output\n",
    "hyp = beam_translate(tok_sentences['en'][0], t, q, ngram_probs['it'], beta=10, min_prob=1.0e-24, max_options=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dd9bc61-63d1-4e92-916b-6e8a74340fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permaculture is a design science based on three simple ethics\n",
      "\t-> su un disegno scienza tre semplici basato permacultura equivale etici\n",
      "\t-> (ref) la permacultura è un metodo di progettazione basato su tre semplici principi etici\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tok_sentences['en'][0]))\n",
    "print('\\t-> ' + ' '.join(hyp[0].translated_sequence[2:]))\n",
    "print('\\t-> (ref) ' + ' '.join(tok_sentences['it'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fe25a-7249-419a-ba64-3139bf7eaddb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f51078d9-c49e-4672-9330-0cca36aa0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(ref_translations:list, translations:list, max_n=4, display=True):\n",
    "    assert len(ref_translations) == len(translations), 'There should be as many reference translations as translations'\n",
    "    \n",
    "    weights_set = [tuple([1/(n) for i in range(n)]) for n in range(1,max_n+1)]\n",
    "    \n",
    "    scores = [bleu.corpus_bleu([[ref] for ref in ref_translations], translations, weights=weights) for weights in weights_set]\n",
    "\n",
    "    if display:\n",
    "        print('Translation scores:')\n",
    "        for i in range(max_n):\n",
    "            print(f'\\t- BLEU-{i+1}: {scores[i]}')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5a4639f-a1d7-4644-8498-25c645a38531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e3262b4b54a35bd40d18c12d01967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translation:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_sentences = tok_sentences['en'][:2000]\n",
    "ref_translated_sentences = tok_sentences['it'][:2000]\n",
    "\n",
    "translated_sentences = []\n",
    "\n",
    "for sent in tqdm(source_sentences, desc='Translation'):\n",
    "    hypotheses = beam_translate(sent, t, q, ngram_probs['it'], beta=5, min_prob=1.0e-7, max_options=20)\n",
    "    \n",
    "    best_hypothesis_sentence = hypotheses[0].translated_sequence[2:]\n",
    "    \n",
    "    translated_sentences.append(best_hypothesis_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c16b85c2-1456-40b9-a375-8f159cced866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0,), (0.5, 0.5), (0.3333333333333333, 0.3333333333333333, 0.3333333333333333), (0.25, 0.25, 0.25, 0.25)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2660981092159196,\n",
       " 0.14394392232122913,\n",
       " 0.08641068038352694,\n",
       " 0.053198782628144106]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_translation(ref_translated_sentences, translated_sentences, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d68177-61f1-422a-aa00-4fcb7d00febc",
   "metadata": {},
   "source": [
    "## Full pipeline (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fffc2a6e-6d63-46cd-b676-e085338296dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_generators = {}\n",
    "\n",
    "for lang in languages:\n",
    "    corpus_generators[lang] = (row.split(' ') for row in open(f'{lang}_corpus_processed.txt', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7a462-005f-4da3-b989-e6c40967a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7f4455-f58f-468c-9cb4-b245ed2cd335",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5715220-42c0-4f39-b330-29c59813bb00",
   "metadata": {},
   "source": [
    "DS:\n",
    "- https://opus.nlpl.eu/OpenSubtitles.php\n",
    "- http://www.opensubtitles.org/\n",
    "\n",
    "General:\n",
    "- https://machinetranslate.org/\n",
    "- https://towardsdatascience.com/machine-translation-b0f0dbcef47c\n",
    "- https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a\n",
    "\n",
    "Evalutation:\n",
    "- https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1\n",
    "\n",
    "Statistical model: \n",
    "- http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf\n",
    "- https://en.wikipedia.org/wiki/IBM_alignment_models\n",
    "- https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/\n",
    "- https://www.youtube.com/watch?v=DuYkqCQEbpo&list=PLQrCiUDqDLG0lQX54o9jB4phJ-SLI6ZBQ\n",
    "- https://www.nltk.org/api/nltk.translate.html\n",
    "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "- https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
